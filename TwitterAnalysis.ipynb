{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('training.1600000.processed.noemoticon.csv', index_col= None, encoding = \"ISO-8859-1\", \n",
    "                      names=['value','views'], usecols=[5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value                                              views\n",
      "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1      0  is upset that he can't update his Facebook by ...\n",
      "2      0  @Kenichan I dived many times for the ball. Man...\n",
      "3      0    my whole body feels itchy and like its on fire \n",
      "4      0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "print(df_temp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df_temp = shuffle(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         value                                              views\n",
      "924636       4  @SEOAly You're welcome, &amp; I hope you're fe...\n",
      "1190240      4  Just made fluffy gluten free pancakes with str...\n",
      "12879        0  @LiziBeeSays I was downtown ALL day yesterday!...\n",
      "1302102      4  @Dannymcfly dannnnny jones  fancy replying? he...\n",
      "72921        0  Why using lucene when iterating is faster? I l...\n",
      "1306774      4              @yayKIMO you are very welcome. &lt;3 \n",
      "1400018      4  @MIMI_loves_YOU yay  lol i got swifty 2 say he...\n",
      "1271792      4  @jennluvs2sing ok, at least i made u smile.  n...\n",
      "1271922      4  @KimSherrell Oh, I just said that I was  looki...\n",
      "1461855      4  is impressed with the current Friendfeed inter...\n",
      "1292714      4  boys boys boys.we like boys in cars.buy us dri...\n"
     ]
    }
   ],
   "source": [
    "print(df_temp[10:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n"
     ]
    }
   ],
   "source": [
    "print(df_temp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "1120000\n",
      "480000\n",
      "1236210    Can't stop watching the trailer!! http://bit.l...\n",
      "1329629    @TheReal_Q He's in Riyadh for God's sake  you ...\n",
      "489002          I'm back! My iPhone had no service up there \n",
      "1233608    @TracieHoward I'm down with you then!   Left h...\n",
      "1574536    Whoo-hoo I just joined today &amp; i'm so happy! \n",
      "Name: views, dtype: object\n",
      "1236210    4\n",
      "1329629    4\n",
      "489002     0\n",
      "1233608    4\n",
      "1574536    4\n",
      "Name: value, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "temp_X = df_temp[\"views\"]\n",
    "temp_y = df_temp['value']\n",
    "print(temp_X.shape[0])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(temp_X, temp_y, test_size=0.3)\n",
    "print(X_train.shape[0])\n",
    "print(X_cv.shape[0])\n",
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_word_list(review, remove_stopwords=False, use_stemmer=False, use_lemmatizer=False):\n",
    "    # 1. Remove HTML. First, we'll remove the HTML tags. For this purpose, \n",
    "    # we'll use the Beautiful Soup\n",
    "    review_text = BeautifulSoup(review,\"lxml\").get_text()\n",
    "    \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    # 5. Optionally use Porter Stemmer\n",
    "    if use_stemmer:\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        words = [porter_stemmer.stem(w) for w in words]\n",
    "    \n",
    "    # 6. Optionally use Lemmatizer\n",
    "    if use_lemmatizer:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        words = [wordnet_lemmatizer.lemmatize(w) for w in words]\n",
    "    #\n",
    "    # 7. Return a list of words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False, use_stemmer=False, use_lemmatizer=False):\n",
    "    # Function to split a review into parsed sentences. Returns a\n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_word_list(raw_sentence,remove_stopwords,use_stemmer, use_lemmatizer))\n",
    "\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "\n",
      "Review 112000 of 1120000\n",
      "\n",
      "Review 224000 of 1120000\n",
      "\n",
      "Review 336000 of 1120000\n",
      "\n",
      "Review 448000 of 1120000\n",
      "\n",
      "Review 560000 of 1120000\n",
      "\n",
      "Review 672000 of 1120000\n",
      "\n",
      "Review 784000 of 1120000\n",
      "\n",
      "Review 896000 of 1120000\n",
      "\n",
      "Review 1008000 of 1120000\n",
      "\n",
      "Review 1120000 of 1120000\n",
      "\n",
      "Complete cleaning review\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews  = X_train.size\n",
    "# print(num_reviews)\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "X_train_val = X_train.values\n",
    "# print(X_train_val[0])\n",
    "\n",
    "\n",
    "print (\"Cleaning and parsing the training set movie reviews...\\n\")\n",
    "for i in range(0, num_reviews):\n",
    "    # If the index is evenly divisible by 5000, print a message\n",
    "    if ((i + 1) % 112000 == 0):\n",
    "        print (\"Review %d of %d\\n\" % (i + 1, num_reviews))\n",
    "    clean_train_reviews.append(review_to_word_list(X_train_val[i], True, True, True))\n",
    "\n",
    "print (\"Complete cleaning review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete initialization of Bag of Words\n"
     ]
    }
   ],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=lambda doc: doc, lowercase=False, preprocessor=None, \n",
    "                             stop_words=None, max_features=700)\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "print('Complete initialization of Bag of Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120000, 700)\n"
     ]
    }
   ],
   "source": [
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "train_data_features = train_data_features.toarray()\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training setfor tag, count in zip(vocab, dist):\n",
    "    # print (count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# print(y_train)\n",
    "\n",
    "print(\"Training the random forest...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100, n_jobs = 6) # number of cores\n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit(train_data_features, y_train.values)\n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Review 80000 of 480000\n",
      "\n",
      "Review 160000 of 480000\n",
      "\n",
      "Review 240000 of 480000\n",
      "\n",
      "Review 320000 of 480000\n",
      "\n",
      "Review 400000 of 480000\n",
      "\n",
      "Review 480000 of 480000\n",
      "\n",
      "completing cross validation review\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = X_cv.size\n",
    "# print(X_cv)\n",
    "\n",
    "clean_crossval_reviews = [] \n",
    "X_cv_val = X_cv.values\n",
    "\n",
    "print (\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1) % 80000 == 0 ):\n",
    "        print (\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
    "    clean_crossval_reviews.append(review_to_word_list(X_cv_val[i], True, True, True))\n",
    "print(\"completing cross validation review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion to numpy array completed.\n"
     ]
    }
   ],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "cv_data_features = vectorizer.transform(clean_crossval_reviews)\n",
    "cv_data_features = cv_data_features.toarray()\n",
    "print('Conversion to numpy array completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71786426  0.7208      0.719592  ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(forest,cv_data_features, y_cv.values)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
