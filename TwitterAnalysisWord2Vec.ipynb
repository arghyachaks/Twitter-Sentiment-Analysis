{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import nltk.data\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('training.1600000.processed.noemoticon.csv', index_col= None, encoding = \"ISO-8859-1\", \n",
    "                      names=['value','views'], usecols=[5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960000\n",
      "640000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df_temp = shuffle(df_temp)\n",
    "\n",
    "temp_X = df_temp[\"views\"]\n",
    "temp_y = df_temp['value']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(temp_X, temp_y, test_size=0.4)\n",
    "print(X_train.shape[0])\n",
    "print(X_cv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_word_list(review, remove_stopwords=False, use_stemmer=False, use_lemmatizer=False):\n",
    "    # 1. Remove HTML. First, we'll remove the HTML tags. For this purpose, \n",
    "    # we'll use the Beautiful Soup\n",
    "    review_text = BeautifulSoup(review,\"lxml\").get_text()\n",
    "    \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    # 5. Optionally use Porter Stemmer\n",
    "    if use_stemmer:\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        words = [porter_stemmer.stem(w) for w in words]\n",
    "    \n",
    "    # 6. Optionally use Lemmatizer\n",
    "    if use_lemmatizer:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        words = [wordnet_lemmatizer.lemmatize(w) for w in words]\n",
    "    #\n",
    "    # 7. Return a list of words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False, use_stemmer=False, use_lemmatizer=False):\n",
    "    # Function to split a review into parsed sentences. Returns a\n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    # raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_word_list(raw_sentence,remove_stopwords,use_stemmer, use_lemmatizer))\n",
    "\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Review 160000 of 960000\n",
      "Review 320000 of 960000\n",
      "Review 480000 of 960000\n",
      "Review 640000 of 960000\n",
      "Review 800000 of 960000\n",
      "Review 960000 of 960000\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "train_X_val = X_train.values\n",
    "i=0\n",
    "num_reviews  = X_train.size\n",
    "print (\"Parsing sentences from training set\")\n",
    "for view in train_X_val:\n",
    "    i = i+1\n",
    "    if ((i + 1) % 160000 == 0):\n",
    "        print (\"Review %d of %d\" % (i + 1, num_reviews))\n",
    "    sentences += review_to_sentences(view, tokenizer)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640841\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'watched', 'one', 'tree', 'hill', 'episode', 'season', 'never', 'cried', 'this', 'much', 'since', 'keith', 'died']\n"
     ]
    }
   ],
   "source": [
    "print (sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 400    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 6       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-05 14:37:16,555 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-01-05 14:37:16,561 : INFO : collecting all words and their counts\n",
      "2017-01-05 14:37:16,561 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-05 14:37:16,588 : INFO : PROGRESS: at sentence #10000, processed 80133 words, keeping 12537 word types\n",
      "2017-01-05 14:37:16,617 : INFO : PROGRESS: at sentence #20000, processed 159331 words, keeping 20020 word types\n",
      "2017-01-05 14:37:16,646 : INFO : PROGRESS: at sentence #30000, processed 240159 words, keeping 26518 word types\n",
      "2017-01-05 14:37:16,674 : INFO : PROGRESS: at sentence #40000, processed 319914 words, keeping 32343 word types\n",
      "2017-01-05 14:37:16,705 : INFO : PROGRESS: at sentence #50000, processed 400163 words, keeping 37733 word types\n",
      "2017-01-05 14:37:16,733 : INFO : PROGRESS: at sentence #60000, processed 479904 words, keeping 42877 word types\n",
      "2017-01-05 14:37:16,764 : INFO : PROGRESS: at sentence #70000, processed 560070 words, keeping 47743 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-05 14:37:16,796 : INFO : PROGRESS: at sentence #80000, processed 640157 words, keeping 52362 word types\n",
      "2017-01-05 14:37:16,826 : INFO : PROGRESS: at sentence #90000, processed 719798 words, keeping 56856 word types\n",
      "2017-01-05 14:37:16,855 : INFO : PROGRESS: at sentence #100000, processed 800416 words, keeping 61264 word types\n",
      "2017-01-05 14:37:16,885 : INFO : PROGRESS: at sentence #110000, processed 879938 words, keeping 65368 word types\n",
      "2017-01-05 14:37:16,915 : INFO : PROGRESS: at sentence #120000, processed 959393 words, keeping 69510 word types\n",
      "2017-01-05 14:37:16,945 : INFO : PROGRESS: at sentence #130000, processed 1038395 words, keeping 73454 word types\n",
      "2017-01-05 14:37:16,974 : INFO : PROGRESS: at sentence #140000, processed 1117411 words, keeping 77404 word types\n",
      "2017-01-05 14:37:17,004 : INFO : PROGRESS: at sentence #150000, processed 1196341 words, keeping 81266 word types\n",
      "2017-01-05 14:37:17,034 : INFO : PROGRESS: at sentence #160000, processed 1276050 words, keeping 85045 word types\n",
      "2017-01-05 14:37:17,067 : INFO : PROGRESS: at sentence #170000, processed 1355627 words, keeping 88704 word types\n",
      "2017-01-05 14:37:17,097 : INFO : PROGRESS: at sentence #180000, processed 1435944 words, keeping 92389 word types\n",
      "2017-01-05 14:37:17,126 : INFO : PROGRESS: at sentence #190000, processed 1514872 words, keeping 95996 word types\n",
      "2017-01-05 14:37:17,157 : INFO : PROGRESS: at sentence #200000, processed 1595153 words, keeping 99417 word types\n",
      "2017-01-05 14:37:17,188 : INFO : PROGRESS: at sentence #210000, processed 1675993 words, keeping 102947 word types\n",
      "2017-01-05 14:37:17,219 : INFO : PROGRESS: at sentence #220000, processed 1756251 words, keeping 106306 word types\n",
      "2017-01-05 14:37:17,249 : INFO : PROGRESS: at sentence #230000, processed 1835308 words, keeping 109608 word types\n",
      "2017-01-05 14:37:17,280 : INFO : PROGRESS: at sentence #240000, processed 1914861 words, keeping 112924 word types\n",
      "2017-01-05 14:37:17,310 : INFO : PROGRESS: at sentence #250000, processed 1993794 words, keeping 116224 word types\n",
      "2017-01-05 14:37:17,340 : INFO : PROGRESS: at sentence #260000, processed 2073744 words, keeping 119413 word types\n",
      "2017-01-05 14:37:17,371 : INFO : PROGRESS: at sentence #270000, processed 2154184 words, keeping 122703 word types\n",
      "2017-01-05 14:37:17,403 : INFO : PROGRESS: at sentence #280000, processed 2233027 words, keeping 125869 word types\n",
      "2017-01-05 14:37:17,433 : INFO : PROGRESS: at sentence #290000, processed 2312410 words, keeping 129018 word types\n",
      "2017-01-05 14:37:17,464 : INFO : PROGRESS: at sentence #300000, processed 2392252 words, keeping 132224 word types\n",
      "2017-01-05 14:37:17,494 : INFO : PROGRESS: at sentence #310000, processed 2471183 words, keeping 135202 word types\n",
      "2017-01-05 14:37:17,525 : INFO : PROGRESS: at sentence #320000, processed 2551635 words, keeping 138235 word types\n",
      "2017-01-05 14:37:17,556 : INFO : PROGRESS: at sentence #330000, processed 2631729 words, keeping 141242 word types\n",
      "2017-01-05 14:37:17,586 : INFO : PROGRESS: at sentence #340000, processed 2711446 words, keeping 144205 word types\n",
      "2017-01-05 14:37:17,619 : INFO : PROGRESS: at sentence #350000, processed 2789965 words, keeping 147289 word types\n",
      "2017-01-05 14:37:17,648 : INFO : PROGRESS: at sentence #360000, processed 2870400 words, keeping 150161 word types\n",
      "2017-01-05 14:37:17,680 : INFO : PROGRESS: at sentence #370000, processed 2950352 words, keeping 153136 word types\n",
      "2017-01-05 14:37:17,711 : INFO : PROGRESS: at sentence #380000, processed 3029680 words, keeping 156081 word types\n",
      "2017-01-05 14:37:17,742 : INFO : PROGRESS: at sentence #390000, processed 3109247 words, keeping 158909 word types\n",
      "2017-01-05 14:37:17,773 : INFO : PROGRESS: at sentence #400000, processed 3189713 words, keeping 161791 word types\n",
      "2017-01-05 14:37:17,810 : INFO : PROGRESS: at sentence #410000, processed 3269272 words, keeping 164499 word types\n",
      "2017-01-05 14:37:17,842 : INFO : PROGRESS: at sentence #420000, processed 3349705 words, keeping 167292 word types\n",
      "2017-01-05 14:37:17,874 : INFO : PROGRESS: at sentence #430000, processed 3429420 words, keeping 170072 word types\n",
      "2017-01-05 14:37:17,906 : INFO : PROGRESS: at sentence #440000, processed 3509179 words, keeping 172796 word types\n",
      "2017-01-05 14:37:17,948 : INFO : PROGRESS: at sentence #450000, processed 3588323 words, keeping 175561 word types\n",
      "2017-01-05 14:37:17,982 : INFO : PROGRESS: at sentence #460000, processed 3668749 words, keeping 178260 word types\n",
      "2017-01-05 14:37:18,016 : INFO : PROGRESS: at sentence #470000, processed 3748321 words, keeping 180932 word types\n",
      "2017-01-05 14:37:18,051 : INFO : PROGRESS: at sentence #480000, processed 3827694 words, keeping 183505 word types\n",
      "2017-01-05 14:37:18,084 : INFO : PROGRESS: at sentence #490000, processed 3908460 words, keeping 186112 word types\n",
      "2017-01-05 14:37:18,116 : INFO : PROGRESS: at sentence #500000, processed 3987296 words, keeping 188717 word types\n",
      "2017-01-05 14:37:18,149 : INFO : PROGRESS: at sentence #510000, processed 4067240 words, keeping 191231 word types\n",
      "2017-01-05 14:37:18,183 : INFO : PROGRESS: at sentence #520000, processed 4147876 words, keeping 193880 word types\n",
      "2017-01-05 14:37:18,215 : INFO : PROGRESS: at sentence #530000, processed 4227343 words, keeping 196365 word types\n",
      "2017-01-05 14:37:18,249 : INFO : PROGRESS: at sentence #540000, processed 4307336 words, keeping 198894 word types\n",
      "2017-01-05 14:37:18,284 : INFO : PROGRESS: at sentence #550000, processed 4387890 words, keeping 201386 word types\n",
      "2017-01-05 14:37:18,317 : INFO : PROGRESS: at sentence #560000, processed 4467787 words, keeping 203919 word types\n",
      "2017-01-05 14:37:18,351 : INFO : PROGRESS: at sentence #570000, processed 4547222 words, keeping 206344 word types\n",
      "2017-01-05 14:37:18,385 : INFO : PROGRESS: at sentence #580000, processed 4626810 words, keeping 208830 word types\n",
      "2017-01-05 14:37:18,419 : INFO : PROGRESS: at sentence #590000, processed 4706663 words, keeping 211353 word types\n",
      "2017-01-05 14:37:18,453 : INFO : PROGRESS: at sentence #600000, processed 4786417 words, keeping 213853 word types\n",
      "2017-01-05 14:37:18,486 : INFO : PROGRESS: at sentence #610000, processed 4867284 words, keeping 216341 word types\n",
      "2017-01-05 14:37:18,519 : INFO : PROGRESS: at sentence #620000, processed 4946951 words, keeping 218774 word types\n",
      "2017-01-05 14:37:18,553 : INFO : PROGRESS: at sentence #630000, processed 5026933 words, keeping 221151 word types\n",
      "2017-01-05 14:37:18,588 : INFO : PROGRESS: at sentence #640000, processed 5107567 words, keeping 223567 word types\n",
      "2017-01-05 14:37:18,623 : INFO : PROGRESS: at sentence #650000, processed 5188131 words, keeping 226030 word types\n",
      "2017-01-05 14:37:18,656 : INFO : PROGRESS: at sentence #660000, processed 5266710 words, keeping 228351 word types\n",
      "2017-01-05 14:37:18,690 : INFO : PROGRESS: at sentence #670000, processed 5345901 words, keeping 230667 word types\n",
      "2017-01-05 14:37:18,724 : INFO : PROGRESS: at sentence #680000, processed 5425450 words, keeping 233104 word types\n",
      "2017-01-05 14:37:18,758 : INFO : PROGRESS: at sentence #690000, processed 5505129 words, keeping 235517 word types\n",
      "2017-01-05 14:37:18,790 : INFO : PROGRESS: at sentence #700000, processed 5584388 words, keeping 237899 word types\n",
      "2017-01-05 14:37:18,839 : INFO : PROGRESS: at sentence #710000, processed 5665241 words, keeping 240263 word types\n",
      "2017-01-05 14:37:18,873 : INFO : PROGRESS: at sentence #720000, processed 5745175 words, keeping 242574 word types\n",
      "2017-01-05 14:37:18,907 : INFO : PROGRESS: at sentence #730000, processed 5825221 words, keeping 244872 word types\n",
      "2017-01-05 14:37:18,940 : INFO : PROGRESS: at sentence #740000, processed 5905016 words, keeping 247155 word types\n",
      "2017-01-05 14:37:18,975 : INFO : PROGRESS: at sentence #750000, processed 5984860 words, keeping 249418 word types\n",
      "2017-01-05 14:37:19,012 : INFO : PROGRESS: at sentence #760000, processed 6065238 words, keeping 251676 word types\n",
      "2017-01-05 14:37:19,046 : INFO : PROGRESS: at sentence #770000, processed 6145160 words, keeping 253923 word types\n",
      "2017-01-05 14:37:19,080 : INFO : PROGRESS: at sentence #780000, processed 6225432 words, keeping 256105 word types\n",
      "2017-01-05 14:37:19,120 : INFO : PROGRESS: at sentence #790000, processed 6306357 words, keeping 258332 word types\n",
      "2017-01-05 14:37:19,156 : INFO : PROGRESS: at sentence #800000, processed 6385494 words, keeping 260457 word types\n",
      "2017-01-05 14:37:19,191 : INFO : PROGRESS: at sentence #810000, processed 6464332 words, keeping 262640 word types\n",
      "2017-01-05 14:37:19,226 : INFO : PROGRESS: at sentence #820000, processed 6543288 words, keeping 264772 word types\n",
      "2017-01-05 14:37:19,261 : INFO : PROGRESS: at sentence #830000, processed 6623350 words, keeping 266951 word types\n",
      "2017-01-05 14:37:19,297 : INFO : PROGRESS: at sentence #840000, processed 6703359 words, keeping 269206 word types\n",
      "2017-01-05 14:37:19,331 : INFO : PROGRESS: at sentence #850000, processed 6783044 words, keeping 271449 word types\n",
      "2017-01-05 14:37:19,369 : INFO : PROGRESS: at sentence #860000, processed 6862360 words, keeping 273574 word types\n",
      "2017-01-05 14:37:19,407 : INFO : PROGRESS: at sentence #870000, processed 6942664 words, keeping 275798 word types\n",
      "2017-01-05 14:37:19,445 : INFO : PROGRESS: at sentence #880000, processed 7022209 words, keeping 278046 word types\n",
      "2017-01-05 14:37:19,483 : INFO : PROGRESS: at sentence #890000, processed 7102334 words, keeping 280210 word types\n",
      "2017-01-05 14:37:19,520 : INFO : PROGRESS: at sentence #900000, processed 7182436 words, keeping 282439 word types\n",
      "2017-01-05 14:37:19,558 : INFO : PROGRESS: at sentence #910000, processed 7261352 words, keeping 284538 word types\n",
      "2017-01-05 14:37:19,595 : INFO : PROGRESS: at sentence #920000, processed 7340988 words, keeping 286741 word types\n",
      "2017-01-05 14:37:19,632 : INFO : PROGRESS: at sentence #930000, processed 7420416 words, keeping 288883 word types\n",
      "2017-01-05 14:37:19,670 : INFO : PROGRESS: at sentence #940000, processed 7501275 words, keeping 290994 word types\n",
      "2017-01-05 14:37:19,706 : INFO : PROGRESS: at sentence #950000, processed 7581228 words, keeping 293068 word types\n",
      "2017-01-05 14:37:19,742 : INFO : PROGRESS: at sentence #960000, processed 7660685 words, keeping 295162 word types\n",
      "2017-01-05 14:37:19,777 : INFO : PROGRESS: at sentence #970000, processed 7740054 words, keeping 297227 word types\n",
      "2017-01-05 14:37:19,812 : INFO : PROGRESS: at sentence #980000, processed 7819248 words, keeping 299334 word types\n",
      "2017-01-05 14:37:19,853 : INFO : PROGRESS: at sentence #990000, processed 7899450 words, keeping 301409 word types\n",
      "2017-01-05 14:37:19,888 : INFO : PROGRESS: at sentence #1000000, processed 7979883 words, keeping 303482 word types\n",
      "2017-01-05 14:37:19,923 : INFO : PROGRESS: at sentence #1010000, processed 8059540 words, keeping 305477 word types\n",
      "2017-01-05 14:37:19,958 : INFO : PROGRESS: at sentence #1020000, processed 8140874 words, keeping 307544 word types\n",
      "2017-01-05 14:37:19,991 : INFO : PROGRESS: at sentence #1030000, processed 8220196 words, keeping 309548 word types\n",
      "2017-01-05 14:37:20,025 : INFO : PROGRESS: at sentence #1040000, processed 8300349 words, keeping 311560 word types\n",
      "2017-01-05 14:37:20,060 : INFO : PROGRESS: at sentence #1050000, processed 8380542 words, keeping 313633 word types\n",
      "2017-01-05 14:37:20,096 : INFO : PROGRESS: at sentence #1060000, processed 8460130 words, keeping 315595 word types\n",
      "2017-01-05 14:37:20,131 : INFO : PROGRESS: at sentence #1070000, processed 8539670 words, keeping 317548 word types\n",
      "2017-01-05 14:37:20,167 : INFO : PROGRESS: at sentence #1080000, processed 8619173 words, keeping 319517 word types\n",
      "2017-01-05 14:37:20,202 : INFO : PROGRESS: at sentence #1090000, processed 8697939 words, keeping 321546 word types\n",
      "2017-01-05 14:37:20,237 : INFO : PROGRESS: at sentence #1100000, processed 8778447 words, keeping 323576 word types\n",
      "2017-01-05 14:37:20,272 : INFO : PROGRESS: at sentence #1110000, processed 8858025 words, keeping 325634 word types\n",
      "2017-01-05 14:37:20,306 : INFO : PROGRESS: at sentence #1120000, processed 8938851 words, keeping 327604 word types\n",
      "2017-01-05 14:37:20,343 : INFO : PROGRESS: at sentence #1130000, processed 9018513 words, keeping 329517 word types\n",
      "2017-01-05 14:37:20,378 : INFO : PROGRESS: at sentence #1140000, processed 9097470 words, keeping 331477 word types\n",
      "2017-01-05 14:37:20,413 : INFO : PROGRESS: at sentence #1150000, processed 9177866 words, keeping 333433 word types\n",
      "2017-01-05 14:37:20,449 : INFO : PROGRESS: at sentence #1160000, processed 9257721 words, keeping 335326 word types\n",
      "2017-01-05 14:37:20,489 : INFO : PROGRESS: at sentence #1170000, processed 9336828 words, keeping 337258 word types\n",
      "2017-01-05 14:37:20,524 : INFO : PROGRESS: at sentence #1180000, processed 9417202 words, keeping 339202 word types\n",
      "2017-01-05 14:37:20,560 : INFO : PROGRESS: at sentence #1190000, processed 9496513 words, keeping 341087 word types\n",
      "2017-01-05 14:37:20,595 : INFO : PROGRESS: at sentence #1200000, processed 9577098 words, keeping 343022 word types\n",
      "2017-01-05 14:37:20,629 : INFO : PROGRESS: at sentence #1210000, processed 9656592 words, keeping 344920 word types\n",
      "2017-01-05 14:37:20,664 : INFO : PROGRESS: at sentence #1220000, processed 9735220 words, keeping 346732 word types\n",
      "2017-01-05 14:37:20,698 : INFO : PROGRESS: at sentence #1230000, processed 9814867 words, keeping 348700 word types\n",
      "2017-01-05 14:37:20,748 : INFO : PROGRESS: at sentence #1240000, processed 9895517 words, keeping 350594 word types\n",
      "2017-01-05 14:37:20,786 : INFO : PROGRESS: at sentence #1250000, processed 9975700 words, keeping 352476 word types\n",
      "2017-01-05 14:37:20,823 : INFO : PROGRESS: at sentence #1260000, processed 10055265 words, keeping 354387 word types\n",
      "2017-01-05 14:37:20,859 : INFO : PROGRESS: at sentence #1270000, processed 10134548 words, keeping 356263 word types\n",
      "2017-01-05 14:37:20,898 : INFO : PROGRESS: at sentence #1280000, processed 10213911 words, keeping 358160 word types\n",
      "2017-01-05 14:37:20,935 : INFO : PROGRESS: at sentence #1290000, processed 10293930 words, keeping 359981 word types\n",
      "2017-01-05 14:37:20,971 : INFO : PROGRESS: at sentence #1300000, processed 10373741 words, keeping 361873 word types\n",
      "2017-01-05 14:37:21,009 : INFO : PROGRESS: at sentence #1310000, processed 10453434 words, keeping 363741 word types\n",
      "2017-01-05 14:37:21,045 : INFO : PROGRESS: at sentence #1320000, processed 10533752 words, keeping 365508 word types\n",
      "2017-01-05 14:37:21,082 : INFO : PROGRESS: at sentence #1330000, processed 10614643 words, keeping 367312 word types\n",
      "2017-01-05 14:37:21,120 : INFO : PROGRESS: at sentence #1340000, processed 10694663 words, keeping 369042 word types\n",
      "2017-01-05 14:37:21,157 : INFO : PROGRESS: at sentence #1350000, processed 10774981 words, keeping 370906 word types\n",
      "2017-01-05 14:37:21,192 : INFO : PROGRESS: at sentence #1360000, processed 10854254 words, keeping 372733 word types\n",
      "2017-01-05 14:37:21,230 : INFO : PROGRESS: at sentence #1370000, processed 10934120 words, keeping 374497 word types\n",
      "2017-01-05 14:37:21,265 : INFO : PROGRESS: at sentence #1380000, processed 11014249 words, keeping 376277 word types\n",
      "2017-01-05 14:37:21,301 : INFO : PROGRESS: at sentence #1390000, processed 11093380 words, keeping 378016 word types\n",
      "2017-01-05 14:37:21,338 : INFO : PROGRESS: at sentence #1400000, processed 11172488 words, keeping 379778 word types\n",
      "2017-01-05 14:37:21,374 : INFO : PROGRESS: at sentence #1410000, processed 11251316 words, keeping 381564 word types\n",
      "2017-01-05 14:37:21,411 : INFO : PROGRESS: at sentence #1420000, processed 11331339 words, keeping 383264 word types\n",
      "2017-01-05 14:37:21,448 : INFO : PROGRESS: at sentence #1430000, processed 11410426 words, keeping 384998 word types\n",
      "2017-01-05 14:37:21,484 : INFO : PROGRESS: at sentence #1440000, processed 11490125 words, keeping 386798 word types\n",
      "2017-01-05 14:37:21,523 : INFO : PROGRESS: at sentence #1450000, processed 11570785 words, keeping 388632 word types\n",
      "2017-01-05 14:37:21,564 : INFO : PROGRESS: at sentence #1460000, processed 11650044 words, keeping 390394 word types\n",
      "2017-01-05 14:37:21,603 : INFO : PROGRESS: at sentence #1470000, processed 11729683 words, keeping 392162 word types\n",
      "2017-01-05 14:37:21,640 : INFO : PROGRESS: at sentence #1480000, processed 11809297 words, keeping 393878 word types\n",
      "2017-01-05 14:37:21,680 : INFO : PROGRESS: at sentence #1490000, processed 11888112 words, keeping 395593 word types\n",
      "2017-01-05 14:37:21,718 : INFO : PROGRESS: at sentence #1500000, processed 11967127 words, keeping 397404 word types\n",
      "2017-01-05 14:37:21,757 : INFO : PROGRESS: at sentence #1510000, processed 12046647 words, keeping 399114 word types\n",
      "2017-01-05 14:37:21,799 : INFO : PROGRESS: at sentence #1520000, processed 12127230 words, keeping 400864 word types\n",
      "2017-01-05 14:37:21,836 : INFO : PROGRESS: at sentence #1530000, processed 12207000 words, keeping 402688 word types\n",
      "2017-01-05 14:37:21,876 : INFO : PROGRESS: at sentence #1540000, processed 12287122 words, keeping 404363 word types\n",
      "2017-01-05 14:37:21,914 : INFO : PROGRESS: at sentence #1550000, processed 12366198 words, keeping 406074 word types\n",
      "2017-01-05 14:37:21,952 : INFO : PROGRESS: at sentence #1560000, processed 12445637 words, keeping 407791 word types\n",
      "2017-01-05 14:37:21,990 : INFO : PROGRESS: at sentence #1570000, processed 12525297 words, keeping 409519 word types\n",
      "2017-01-05 14:37:22,031 : INFO : PROGRESS: at sentence #1580000, processed 12606304 words, keeping 411286 word types\n",
      "2017-01-05 14:37:22,069 : INFO : PROGRESS: at sentence #1590000, processed 12684810 words, keeping 412962 word types\n",
      "2017-01-05 14:37:22,106 : INFO : PROGRESS: at sentence #1600000, processed 12764714 words, keeping 414619 word types\n",
      "2017-01-05 14:37:22,147 : INFO : PROGRESS: at sentence #1610000, processed 12844296 words, keeping 416266 word types\n",
      "2017-01-05 14:37:22,186 : INFO : PROGRESS: at sentence #1620000, processed 12924595 words, keeping 417911 word types\n",
      "2017-01-05 14:37:22,225 : INFO : PROGRESS: at sentence #1630000, processed 13004727 words, keeping 419616 word types\n",
      "2017-01-05 14:37:22,266 : INFO : PROGRESS: at sentence #1640000, processed 13084993 words, keeping 421381 word types\n",
      "2017-01-05 14:37:22,280 : INFO : collected 421528 word types from a corpus of 13091556 raw words and 1640841 sentences\n",
      "2017-01-05 14:37:22,281 : INFO : Loading a fresh vocabulary\n",
      "2017-01-05 14:37:22,541 : INFO : min_count=40 retains 11418 unique words (2% of original 421528, drops 410110)\n",
      "2017-01-05 14:37:22,541 : INFO : min_count=40 leaves 12083523 word corpus (92% of original 13091556, drops 1008033)\n",
      "2017-01-05 14:37:22,600 : INFO : deleting the raw counts dictionary of 421528 items\n",
      "2017-01-05 14:37:22,617 : INFO : sample=0.001 downsamples 59 most-common words\n",
      "2017-01-05 14:37:22,618 : INFO : downsampling leaves estimated 9245584 word corpus (76.5% of prior 12083523)\n",
      "2017-01-05 14:37:22,619 : INFO : estimated required memory for 11418 words and 300 dimensions: 33112200 bytes\n",
      "2017-01-05 14:37:22,666 : INFO : resetting layer weights\n",
      "2017-01-05 14:37:22,969 : INFO : training model with 6 workers on 11418 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-01-05 14:37:22,970 : INFO : expecting 1640841 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-05 14:37:24,032 : INFO : PROGRESS: at 0.56% examples, 248253 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:25,054 : INFO : PROGRESS: at 1.20% examples, 268508 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:26,103 : INFO : PROGRESS: at 1.82% examples, 268811 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:27,161 : INFO : PROGRESS: at 2.35% examples, 259929 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:37:28,191 : INFO : PROGRESS: at 2.82% examples, 250617 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:37:29,205 : INFO : PROGRESS: at 3.31% examples, 246152 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:30,211 : INFO : PROGRESS: at 3.93% examples, 250962 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:31,235 : INFO : PROGRESS: at 4.57% examples, 255775 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:32,254 : INFO : PROGRESS: at 5.21% examples, 259674 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:33,303 : INFO : PROGRESS: at 5.85% examples, 262027 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:34,315 : INFO : PROGRESS: at 6.45% examples, 262907 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:37:35,334 : INFO : PROGRESS: at 6.90% examples, 258396 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:36,340 : INFO : PROGRESS: at 7.36% examples, 254749 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:37,351 : INFO : PROGRESS: at 7.82% examples, 251579 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:38,367 : INFO : PROGRESS: at 8.28% examples, 248735 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:39,384 : INFO : PROGRESS: at 8.73% examples, 246202 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:37:40,393 : INFO : PROGRESS: at 9.19% examples, 244060 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:41,406 : INFO : PROGRESS: at 9.69% examples, 243299 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:42,424 : INFO : PROGRESS: at 10.33% examples, 245802 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:43,441 : INFO : PROGRESS: at 10.94% examples, 247375 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:44,465 : INFO : PROGRESS: at 11.59% examples, 249379 words/s, in_qsize 10, out_qsize 1\n",
      "2017-01-05 14:37:45,479 : INFO : PROGRESS: at 12.23% examples, 251320 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:46,502 : INFO : PROGRESS: at 12.82% examples, 252089 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:37:47,503 : INFO : PROGRESS: at 13.27% examples, 250151 words/s, in_qsize 11, out_qsize 1\n",
      "2017-01-05 14:37:48,522 : INFO : PROGRESS: at 13.80% examples, 249828 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:49,525 : INFO : PROGRESS: at 14.41% examples, 251038 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:50,537 : INFO : PROGRESS: at 14.87% examples, 249504 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:37:51,544 : INFO : PROGRESS: at 15.33% examples, 248117 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:52,552 : INFO : PROGRESS: at 15.79% examples, 246797 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:37:53,589 : INFO : PROGRESS: at 16.32% examples, 246516 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:54,602 : INFO : PROGRESS: at 16.82% examples, 245990 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:55,609 : INFO : PROGRESS: at 17.31% examples, 245338 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:56,621 : INFO : PROGRESS: at 17.77% examples, 244247 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:37:57,664 : INFO : PROGRESS: at 18.32% examples, 244211 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:37:58,693 : INFO : PROGRESS: at 18.96% examples, 245471 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:37:59,764 : INFO : PROGRESS: at 19.59% examples, 246191 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:00,778 : INFO : PROGRESS: at 20.22% examples, 247239 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:01,786 : INFO : PROGRESS: at 20.85% examples, 248441 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:02,789 : INFO : PROGRESS: at 21.46% examples, 249281 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:03,798 : INFO : PROGRESS: at 22.05% examples, 249704 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:04,802 : INFO : PROGRESS: at 22.49% examples, 248600 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:05,822 : INFO : PROGRESS: at 22.95% examples, 247630 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:06,831 : INFO : PROGRESS: at 23.41% examples, 246760 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:07,836 : INFO : PROGRESS: at 23.87% examples, 245954 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:08,844 : INFO : PROGRESS: at 24.40% examples, 245938 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:09,893 : INFO : PROGRESS: at 25.04% examples, 246765 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:10,906 : INFO : PROGRESS: at 25.53% examples, 246263 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:11,923 : INFO : PROGRESS: at 25.99% examples, 245475 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:12,951 : INFO : PROGRESS: at 26.45% examples, 244665 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:13,989 : INFO : PROGRESS: at 27.06% examples, 245234 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:15,020 : INFO : PROGRESS: at 27.64% examples, 245525 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:16,024 : INFO : PROGRESS: at 28.12% examples, 245133 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:17,030 : INFO : PROGRESS: at 28.58% examples, 244477 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:18,088 : INFO : PROGRESS: at 29.19% examples, 244899 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:19,105 : INFO : PROGRESS: at 29.82% examples, 245621 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:20,115 : INFO : PROGRESS: at 30.28% examples, 244982 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:21,119 : INFO : PROGRESS: at 30.76% examples, 244628 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:22,134 : INFO : PROGRESS: at 31.22% examples, 244012 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:23,153 : INFO : PROGRESS: at 31.68% examples, 243401 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:24,170 : INFO : PROGRESS: at 32.20% examples, 243274 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:25,181 : INFO : PROGRESS: at 32.79% examples, 243738 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:26,194 : INFO : PROGRESS: at 33.40% examples, 244300 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:27,251 : INFO : PROGRESS: at 33.88% examples, 243689 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:28,260 : INFO : PROGRESS: at 34.43% examples, 243814 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:29,305 : INFO : PROGRESS: at 34.95% examples, 243583 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:30,333 : INFO : PROGRESS: at 35.48% examples, 243528 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:31,338 : INFO : PROGRESS: at 36.11% examples, 244181 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:32,352 : INFO : PROGRESS: at 36.73% examples, 244787 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:33,357 : INFO : PROGRESS: at 37.19% examples, 244305 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:34,372 : INFO : PROGRESS: at 37.65% examples, 243795 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:35,385 : INFO : PROGRESS: at 38.11% examples, 243302 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:36,393 : INFO : PROGRESS: at 38.64% examples, 243319 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:37,417 : INFO : PROGRESS: at 39.15% examples, 243104 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:38,428 : INFO : PROGRESS: at 39.77% examples, 243687 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:39,438 : INFO : PROGRESS: at 40.38% examples, 244153 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:40,441 : INFO : PROGRESS: at 40.99% examples, 244633 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:41,481 : INFO : PROGRESS: at 41.56% examples, 244720 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:42,490 : INFO : PROGRESS: at 42.02% examples, 244278 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:43,496 : INFO : PROGRESS: at 42.48% examples, 243860 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:44,505 : INFO : PROGRESS: at 42.93% examples, 243441 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:45,522 : INFO : PROGRESS: at 43.39% examples, 243011 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:46,531 : INFO : PROGRESS: at 43.85% examples, 242611 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:47,538 : INFO : PROGRESS: at 44.31% examples, 242229 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:48,538 : INFO : PROGRESS: at 44.78% examples, 241956 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:49,542 : INFO : PROGRESS: at 45.38% examples, 242334 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:50,652 : INFO : PROGRESS: at 45.97% examples, 242406 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:51,659 : INFO : PROGRESS: at 46.60% examples, 242923 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:52,665 : INFO : PROGRESS: at 47.07% examples, 242639 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:53,679 : INFO : PROGRESS: at 47.62% examples, 242724 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:54,697 : INFO : PROGRESS: at 48.20% examples, 242952 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:55,720 : INFO : PROGRESS: at 48.66% examples, 242552 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:38:56,763 : INFO : PROGRESS: at 49.12% examples, 242115 words/s, in_qsize 12, out_qsize 2\n",
      "2017-01-05 14:38:57,770 : INFO : PROGRESS: at 49.69% examples, 242372 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:58,801 : INFO : PROGRESS: at 50.32% examples, 242785 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:38:59,827 : INFO : PROGRESS: at 50.95% examples, 243196 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:00,880 : INFO : PROGRESS: at 51.57% examples, 243534 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:01,889 : INFO : PROGRESS: at 52.06% examples, 243333 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:02,902 : INFO : PROGRESS: at 52.52% examples, 242985 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:03,923 : INFO : PROGRESS: at 52.99% examples, 242696 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:04,943 : INFO : PROGRESS: at 53.45% examples, 242344 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:05,976 : INFO : PROGRESS: at 54.06% examples, 242660 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:06,982 : INFO : PROGRESS: at 54.52% examples, 242348 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:07,991 : INFO : PROGRESS: at 54.98% examples, 242039 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:09,014 : INFO : PROGRESS: at 55.44% examples, 241701 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:10,083 : INFO : PROGRESS: at 56.06% examples, 241983 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:11,090 : INFO : PROGRESS: at 56.67% examples, 242341 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:12,100 : INFO : PROGRESS: at 57.28% examples, 242689 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:13,115 : INFO : PROGRESS: at 57.76% examples, 242436 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:14,179 : INFO : PROGRESS: at 58.37% examples, 242652 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:15,186 : INFO : PROGRESS: at 58.83% examples, 242362 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:16,188 : INFO : PROGRESS: at 59.38% examples, 242460 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:17,226 : INFO : PROGRESS: at 59.97% examples, 242666 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:18,282 : INFO : PROGRESS: at 60.54% examples, 242708 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:19,293 : INFO : PROGRESS: at 61.13% examples, 242962 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:20,342 : INFO : PROGRESS: at 61.71% examples, 243082 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:21,358 : INFO : PROGRESS: at 62.17% examples, 242784 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:22,365 : INFO : PROGRESS: at 62.63% examples, 242507 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:23,380 : INFO : PROGRESS: at 63.23% examples, 242751 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:24,420 : INFO : PROGRESS: at 63.84% examples, 242994 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:25,430 : INFO : PROGRESS: at 64.45% examples, 243299 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:26,446 : INFO : PROGRESS: at 65.05% examples, 243526 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:27,477 : INFO : PROGRESS: at 65.64% examples, 243720 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:28,493 : INFO : PROGRESS: at 66.27% examples, 244055 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:29,496 : INFO : PROGRESS: at 66.75% examples, 243906 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:30,509 : INFO : PROGRESS: at 67.21% examples, 243629 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:31,524 : INFO : PROGRESS: at 67.67% examples, 243353 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:32,542 : INFO : PROGRESS: at 68.13% examples, 243077 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:33,551 : INFO : PROGRESS: at 68.65% examples, 243032 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:34,553 : INFO : PROGRESS: at 69.24% examples, 243274 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:35,556 : INFO : PROGRESS: at 69.74% examples, 243192 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:36,562 : INFO : PROGRESS: at 70.37% examples, 243528 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:37,577 : INFO : PROGRESS: at 70.98% examples, 243789 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:38,587 : INFO : PROGRESS: at 71.51% examples, 243794 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:39,595 : INFO : PROGRESS: at 71.97% examples, 243545 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:40,616 : INFO : PROGRESS: at 72.43% examples, 243277 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:41,640 : INFO : PROGRESS: at 72.89% examples, 243010 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:42,647 : INFO : PROGRESS: at 73.35% examples, 242776 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:43,654 : INFO : PROGRESS: at 73.79% examples, 242492 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:44,678 : INFO : PROGRESS: at 74.25% examples, 242236 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:45,691 : INFO : PROGRESS: at 74.80% examples, 242297 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:46,703 : INFO : PROGRESS: at 75.28% examples, 242163 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:47,726 : INFO : PROGRESS: at 75.74% examples, 241910 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:48,749 : INFO : PROGRESS: at 76.20% examples, 241669 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:39:49,765 : INFO : PROGRESS: at 76.83% examples, 241969 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:50,835 : INFO : PROGRESS: at 77.47% examples, 242226 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:51,847 : INFO : PROGRESS: at 78.10% examples, 242520 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:52,857 : INFO : PROGRESS: at 78.72% examples, 242814 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:53,874 : INFO : PROGRESS: at 79.30% examples, 242957 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:54,876 : INFO : PROGRESS: at 79.88% examples, 243121 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:55,880 : INFO : PROGRESS: at 80.48% examples, 243326 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:56,891 : INFO : PROGRESS: at 80.95% examples, 243147 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:57,894 : INFO : PROGRESS: at 81.56% examples, 243395 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:39:58,911 : INFO : PROGRESS: at 82.13% examples, 243484 words/s, in_qsize 10, out_qsize 1\n",
      "2017-01-05 14:39:59,922 : INFO : PROGRESS: at 82.71% examples, 243623 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:00,933 : INFO : PROGRESS: at 83.17% examples, 243407 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:01,942 : INFO : PROGRESS: at 83.72% examples, 243459 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:02,973 : INFO : PROGRESS: at 84.33% examples, 243656 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:03,999 : INFO : PROGRESS: at 84.88% examples, 243686 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:05,003 : INFO : PROGRESS: at 85.37% examples, 243571 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:40:06,065 : INFO : PROGRESS: at 85.98% examples, 243716 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:07,078 : INFO : PROGRESS: at 86.45% examples, 243544 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:40:08,112 : INFO : PROGRESS: at 86.92% examples, 243346 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:09,112 : INFO : PROGRESS: at 87.38% examples, 243156 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:40:10,207 : INFO : PROGRESS: at 87.85% examples, 242871 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:40:11,207 : INFO : PROGRESS: at 88.31% examples, 242683 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:12,254 : INFO : PROGRESS: at 88.78% examples, 242473 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:40:13,294 : INFO : PROGRESS: at 89.41% examples, 242690 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:14,308 : INFO : PROGRESS: at 90.02% examples, 242905 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:15,356 : INFO : PROGRESS: at 90.65% examples, 243103 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:16,364 : INFO : PROGRESS: at 91.27% examples, 243356 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:17,378 : INFO : PROGRESS: at 91.85% examples, 243480 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:18,409 : INFO : PROGRESS: at 92.48% examples, 243700 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:19,439 : INFO : PROGRESS: at 93.10% examples, 243919 words/s, in_qsize 10, out_qsize 1\n",
      "2017-01-05 14:40:20,470 : INFO : PROGRESS: at 93.74% examples, 244171 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:21,515 : INFO : PROGRESS: at 94.36% examples, 244325 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:22,516 : INFO : PROGRESS: at 94.94% examples, 244457 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:23,539 : INFO : PROGRESS: at 95.41% examples, 244283 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:24,553 : INFO : PROGRESS: at 95.87% examples, 244081 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:25,624 : INFO : PROGRESS: at 96.35% examples, 243889 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:26,641 : INFO : PROGRESS: at 96.81% examples, 243691 words/s, in_qsize 11, out_qsize 0\n",
      "2017-01-05 14:40:27,657 : INFO : PROGRESS: at 97.27% examples, 243498 words/s, in_qsize 12, out_qsize 1\n",
      "2017-01-05 14:40:28,669 : INFO : PROGRESS: at 97.75% examples, 243349 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:40:29,684 : INFO : PROGRESS: at 98.21% examples, 243157 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:40:30,698 : INFO : PROGRESS: at 98.67% examples, 242971 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:40:31,758 : INFO : PROGRESS: at 99.14% examples, 242768 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:40:32,761 : INFO : PROGRESS: at 99.69% examples, 242822 words/s, in_qsize 12, out_qsize 0\n",
      "2017-01-05 14:40:33,163 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-05 14:40:33,200 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-05 14:40:33,209 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-05 14:40:33,216 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-05 14:40:33,224 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-05 14:40:33,234 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-05 14:40:33,234 : INFO : training on 65457780 raw words (46227878 effective words) took 190.3s, 242977 effective words/s\n",
      "2017-01-05 14:40:33,235 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-01-05 14:40:33,399 : INFO : saving Word2Vec object under 400features_40minwords_10context, separately None\n",
      "2017-01-05 14:40:33,400 : INFO : not storing attribute cum_table\n",
      "2017-01-05 14:40:33,401 : INFO : not storing attribute syn0norm\n",
      "2017-01-05 14:40:33,636 : INFO : saved 400features_40minwords_10context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Training\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, \n",
    "                          min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"400features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "\n",
    "print('Completed Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guy', 0.547832727432251),\n",
       " ('boy', 0.5378686189651489),\n",
       " ('dude', 0.5372099876403809),\n",
       " ('woman', 0.5245792865753174),\n",
       " ('girl', 0.4487118721008301),\n",
       " ('gosh', 0.44019967317581177),\n",
       " ('kid', 0.438829243183136),\n",
       " ('nigga', 0.4201546013355255),\n",
       " ('bastard', 0.4112083911895752),\n",
       " ('chick', 0.4029228091239929)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-05 14:48:58,192 : INFO : loading Word2Vec object from 400features_40minwords_10context\n",
      "2017-01-05 14:48:58,396 : INFO : setting ignored attribute cum_table to None\n",
      "2017-01-05 14:48:58,397 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-01-05 14:48:58,398 : INFO : loaded 400features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Load the module\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"400features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    \n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(views, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    viewFeatureVecs = np.zeros((len(views),num_features),dtype=\"float32\")\n",
    "    \n",
    "    # Loop through the reviews\n",
    "    for view in views:\n",
    "        # Print a status message every 5000th review\n",
    "        if counter%30000. == 0.:\n",
    "           print (\"Review %d of %d\" % (counter, len(views)))\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        viewFeatureVecs[counter] = makeFeatureVec(view, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    \n",
    "    return viewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for train views\n",
      "Review 0 of 960000\n",
      "Review 30000 of 960000\n",
      "Review 60000 of 960000\n",
      "Review 90000 of 960000\n",
      "Review 120000 of 960000\n",
      "Review 150000 of 960000\n",
      "Review 180000 of 960000\n",
      "Review 210000 of 960000\n",
      "Review 240000 of 960000\n",
      "Review 270000 of 960000\n",
      "Review 300000 of 960000\n",
      "Review 330000 of 960000\n",
      "Review 360000 of 960000\n",
      "Review 390000 of 960000\n",
      "Review 420000 of 960000\n",
      "Review 450000 of 960000\n",
      "Review 480000 of 960000\n",
      "Review 510000 of 960000\n",
      "Review 540000 of 960000\n",
      "Review 570000 of 960000\n",
      "Review 600000 of 960000\n",
      "Review 630000 of 960000\n",
      "Review 660000 of 960000\n",
      "Review 690000 of 960000\n",
      "Review 720000 of 960000\n",
      "Review 750000 of 960000\n",
      "Review 780000 of 960000\n",
      "Review 810000 of 960000\n",
      "Review 840000 of 960000\n",
      "Review 870000 of 960000\n",
      "Review 900000 of 960000\n",
      "Review 930000 of 960000\n",
      "Complete average feature vector\n"
     ]
    }
   ],
   "source": [
    "# Calculate average feature vectors for training and testing sets, using the functions we defined above. \n",
    "# Notice that we now use stop word removal.\n",
    "print (\"Creating average feature vecs for train views\")\n",
    "clean_train_views = []\n",
    "for view in train_X_val:\n",
    "    clean_train_views.append( review_to_word_list( view,remove_stopwords=True ))\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_views, model, num_features )\n",
    "\n",
    "print('Complete average feature vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_indices = np.where(np.isnan(trainDataVecs))\n",
    "trainDataVecs[bad_indices] = 0\n",
    "# print(bad_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the random forest...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100, n_jobs = 6) # number of cores\n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit(trainDataVecs, y_train.values)\n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('testdata.manual.2009.06.14.csv', index_col= None, encoding = \"ISO-8859-1\", \n",
    "                      names=['value','views'], usecols=[5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value                                              views\n",
      "0      4  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
      "1      4  Reading my kindle2...  Love it... Lee childs i...\n",
      "2      4  Ok, first assesment of the #kindle2 ...it fuck...\n",
      "3      4  @kenburbary You'll love your Kindle2. I've had...\n",
      "4      4  @mikefish  Fair enough. But i have the Kindle2...\n"
     ]
    }
   ],
   "source": [
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n"
     ]
    }
   ],
   "source": [
    "test_X = df_test[\"views\"]\n",
    "test_y = df_test['value']\n",
    "print(test_X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 498\n",
      "Completing feature creation\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating average feature vecs for test reviews\")\n",
    "X_test_val = test_X.values\n",
    "clean_test_reviews = []\n",
    "for review in X_test_val:\n",
    "    clean_test_reviews.append( review_to_word_list( review, remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )\n",
    "\n",
    "print('Completing feature creation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_indices = np.where(np.isnan(testDataVecs))\n",
    "testDataVecs[bad_indices] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62275449  0.6626506   0.63030303]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "test_scores = cross_val_score(forest,testDataVecs, test_y.values)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
